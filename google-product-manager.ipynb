{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Google Product Manager Project <a class=\"anchor\" id=\"top\"></a>\n",
    "\n",
    "## Table of contents: \n",
    "* [Goal](#goal)\n",
    "* [Method](#method)\n",
    "* [Requirements](#requirements)\n",
    "* [Process](#process)\n",
    "    * [Authenticate](#auth)\n",
    "    * [List issues](#list)\n",
    "    * [Corpus](#corpus)\n",
    "    * [Cluster](#cluster)\n",
    "    * [Prioritize](#prioritize)\n",
    "\n",
    "## Goal <a class=\"anchor\" id=\"goal\"></a>\n",
    "\n",
    "The goal of this Jupyter notebook is to mine the information from Google's issue tracker to produce two prioritized list of features and bugs.\n",
    "\n",
    "[Go to top](#top)\n",
    "\n",
    "## Method <a class=\"anchor\" id=\"method\"></a>\n",
    "\n",
    "The following is a high-level strategy of attaining the above goal:\n",
    "\n",
    "1. Mine google's public issue tracker for raw data for features and bugs (the next steps are repeated for each)\n",
    "2. Extract the \"description\" of the issue.\n",
    "3. Use a vectorizer or natural language library to extract the text features (bigrams and unigrams)\n",
    "4. Use a unsupervised clustering algorithm to identify clusters of issues.\n",
    "5. Prioritize the top 3 clusters using an index generated by the number of issues and votes.\n",
    "6. Identify the scenario (W5H) for each cluster.\n",
    "7. Identify the persona for each cluster.\n",
    "8. Identify stories for each cluster.\n",
    "9. Priorize the stories.\n",
    "\n",
    "[Go to top](#top)\n",
    "\n",
    "## Requirements <a class=\"anchor\" id=\"requirements\"></a>\n",
    "\n",
    "1. Clone this repository.\n",
    "2. Ensure you have python3 and jupyter-notebook installed on your system.\n",
    "3. Create an virtual environment.\n",
    "4. Activate the virtual environment.\n",
    "5. Install the dependencies in the requirements.txt file.\n",
    "5. Run the command jupyter-notebook.\n",
    "6. Open the google-product-manager.ipynb document.\n",
    "\n",
    "[Go to top](#top)\n",
    "\n",
    "## Process <a class=\"anchor\" id=\"process\"></a>\n",
    "\n",
    "### Authenticate <a class=\"anchor\" id=\"auth\"></a>\n",
    "\n",
    "Google's issue tracker is public.  It allows anyone to report issues and request features.  The issue tracker does not have a supported API or SDK (that I can tell), but when loading the pages with developer mode enabled I can find a request that returns a JSON structure with the details necessary to do some kind of post-processing.\n",
    "\n",
    "When looking at the issue tracker in development mode, we can see that a XHR request is made using the following URL:\n",
    "\n",
    "* Bugs: https://issuetracker.google.com/action/issues?count=25&p=1&q=status:open+type:bug&s=created_time:desc\n",
    "* Features: https://issuetracker.google.com/action/issues?count=25&p=1&q=status:open+type:feature_request&s=created_time:desc\n",
    "\n",
    "Since the HTTP request requires authentication and there is no official API or SDK for the issue tracker, I used a browsercookie library in python to bypass the authentication piece (I tried using APIs, but had difficulty)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Firefox session filename does not exist: /home/daniel/.mozilla/firefox/edv7ja09.default/sessionstore.js\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import browsercookie\n",
    "import json\n",
    "\n",
    "cookies = browsercookie.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to top](#top)\n",
    "\n",
    "### Get list of issues <a class=\"anchor\" id=\"list\"></a>\n",
    "\n",
    "After looking at the response, I found several caveats:\n",
    "* The response is multi-line and the first line is not valid json.\n",
    "* The syntax required the character \"+\" which the python requests library encodes by default.\n",
    "\n",
    "I created two queries that would produce a list of bugs and issues sorted by their creation date.  I then wrote some code that would pull down a list of issues for each query and work around some of these limitations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def pull_issues(issue_type, issue_limit=5000):\n",
    "    \n",
    "    generic_url = \"https://issuetracker.google.com/action/issues?count=999&p={page}&q=status:open+type:{issue_type}&s=created_time:desc\"\n",
    "    \n",
    "    assert issue_type in [\"feature_request\", \"bug\"]\n",
    "    page = 1\n",
    "    collection = []\n",
    "    \n",
    "    while(True):\n",
    "        url = generic_url.format(issue_type=issue_type, page=page)\n",
    "        response = requests.get(url, cookies=cookies)\n",
    "        lines = [x for x in response.iter_lines()] # workaround for first line.\n",
    "        \n",
    "        data = json.loads(lines[1].decode())\n",
    "    \n",
    "        if data.get('issues'):\n",
    "            if len(data['issues']) == 0:\n",
    "                break\n",
    "            elif len(data['issues']) >= issue_limit:\n",
    "                break\n",
    "            else:\n",
    "                collection.extend(data['issues'])\n",
    "                page += 1\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return collection[:issue_limit]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the function to pull down the bug and feature lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5000 bugs.\n",
      "Found 5000 features.\n"
     ]
    }
   ],
   "source": [
    "bugs = pull_issues('bug')\n",
    "features = pull_issues('feature_request')\n",
    "\n",
    "print(\"Found {} bugs.\".format(len(bugs)))\n",
    "print(\"Found {} features.\".format(len(features)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Go to top](#top)\n",
    "\n",
    "### Corpus <a class=\"anchor\" id=\"corpus\"></a>\n",
    "\n",
    "If we inspect a single issue, we can take a look at the some the data within, but the only text data that we see is the title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checksums for private Compute Engine Images\n"
     ]
    }
   ],
   "source": [
    "print(features[0]['snapshot'][0]['title'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that we are able to open each issue using a similar request that makes use of the issue ID.  An example for the issue above is \n",
    "\n",
    "https://issuetracker.google.com/action/issues/74163608\n",
    "\n",
    "The structure looks like the zero-indexed comment or the initial comment is similar to a verbose description of the problem or feature request.  Let's try making a HTTP request using the same trick as above to get a description for each issue..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is it possible to get the feature added to get individual sizing for each page on a datastudio report.<br>We have one page on a report that requires 900+ length and another that requires 600 to avoid scrolling an empty canvas.<br><br>This will be especially useful when it comes to embedding pages on a site or elsewhere for readability.\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(\"https://issuetracker.google.com/action/issues/74163608\", cookies=cookies)\n",
    "lines = [x for x in response.iter_lines()]\n",
    "the_issue = json.loads(lines[1].decode())\n",
    "\n",
    "print(the_issue['events'][0]['comment'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have some text, we can try to cluster the text to see what groups come out of it.  Will be using this guide as inspiration: https://pythonprogramminglanguage.com/kmeans-text-clustering/\n",
    "\n",
    "Let's start by creating a corpus to cluster and cleaning up the text so that it does not contain HTML or escaped characters.  \n",
    "\n",
    "**WARNING:** This operation is very time consuming, so I have written it in a way that the results are stored in local text files.  If you want to limit the number of issues you are clustering, please add a range in the \"for feature in features:\" line; for example, \"for feature in features[:5]:\" will only download the details for 5 issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4990 saved documents.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import html\n",
    "import pickle\n",
    "\n",
    "try:\n",
    "    # Attempt to open a pre-existing file with feature descriptions.\n",
    "    with open('feature_documents.pickle', 'rb') as f:\n",
    "        feature_documents = pickle.load(f)\n",
    "        \n",
    "    print(\"Found {} saved documents.\".format(len(feature_documents)))\n",
    "\n",
    "except FileNotFoundError:\n",
    "\n",
    "    # If a file does not exist, then pull it manually.\n",
    "    feature_documents = []\n",
    "    \n",
    "    for feature in features:\n",
    "        url = \"https://issuetracker.google.com/action/issues/\" + str(feature['issueId'])\n",
    "        response = requests.get(url, cookies=cookies)\n",
    "        lines = [x for x in response.iter_lines()]\n",
    "        try:\n",
    "            first_comment = json.loads(lines[1].decode())['events'][0]['comment']\n",
    "            document = re.sub('<[^<]+?>', '', first_comment) # There appears to be HTML formatting support in comments.\n",
    "            document = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', document, flags=re.MULTILINE) # Remove URLs.\n",
    "            document = html.unescape(document)\n",
    "            feature_documents.append(document)\n",
    "        except KeyError:\n",
    "            continue\n",
    "        \n",
    "    with open('feature_documents.pickle', 'wb') as f:\n",
    "        pickle.dump(feature_documents, f)\n",
    "        \n",
    "    print(\"Downloaded and saved {} documents.\".format(len(feature_documents)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to top](#top)\n",
    "### Cluster  <a class=\"anchor\" id=\"cluster\"></a>\n",
    "\n",
    "Now, lets apply some of the machine learning k-means clustering goodness to produce some a list of top 10 features in each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top terms per feature cluster for the first 3 clusters:\n",
      "\n",
      "Feature cluster 0:\n",
      " connect\n",
      " mesh\n",
      " able connect\n",
      " sql connect\n",
      " heroku\n",
      " heroku postgres\n",
      " mesh network\n",
      " client\n",
      " gcloud sql\n",
      " connection\n",
      "\n",
      "Feature cluster 1:\n",
      " java\n",
      " 171\n",
      " ai 171\n",
      " ai\n",
      " tab\n",
      " rows\n",
      " 1920x1080\n",
      " android\n",
      " 171 4474551\n",
      " 4474551\n",
      "\n",
      "Feature cluster 2:\n",
      " maker\n",
      " app maker\n",
      " app\n",
      " height\n",
      " group\n",
      " preview\n",
      " visibility\n",
      " collapsible\n",
      " property\n",
      " panel\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "\n",
    "stop_words = text.ENGLISH_STOP_WORDS.union([\"feature\", \"features\", \"issue\", \"issues\", \"requests\", \"requests\", \"thing\", \"things\"])\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words, ngram_range=(1,2))\n",
    "x = vectorizer.fit_transform(feature_documents)\n",
    "\n",
    "number_of_clusters = 50\n",
    "feature_model = KMeans(n_clusters=number_of_clusters, init='k-means++', max_iter=100, n_init=1)\n",
    "feature_model.fit(x)\n",
    "\n",
    "print(\"Top terms per feature cluster for the first 3 clusters:\")\n",
    "print()\n",
    "order_centroids = feature_model.cluster_centers_.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i in range(3):\n",
    "    print(\"Feature cluster %d:\" % i),\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(' %s' % terms[ind]),\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I can make predictions of which cluster a string belongs to..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The following command belongs in cluster 25:\n",
      "Issue summary: Provide support for multi regional Managed instance groups Business impact for users: wants to ensure cookie session affinity stays intact, even if client changes IP, uses a proxy or a VPN. Task the customer wishes to accomplish: Either have multi-regional instance groups, or have session affinity take precedence over the LB location algorithm. Current functionality if applicable: Multi-zone Current customer workaround(s): none, must create multiple managed instance groups and have multiple backend services\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "idx = random.randint(0, len(features))\n",
    "\n",
    "url = \"https://issuetracker.google.com/action/issues/\" + str(features[idx]['issueId'])\n",
    "response = requests.get(url, cookies=cookies)\n",
    "lines = [x for x in response.iter_lines()]\n",
    "first_comment = json.loads(lines[1].decode())['events'][0]['comment']\n",
    "document = re.sub('<[^<]+?>', '', first_comment) # There appears to be HTML formatting support in comments.\n",
    "document = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', document, flags=re.MULTILINE) # Remove URLs.\n",
    "document = html.unescape(document)\n",
    "\n",
    "y = vectorizer.transform([document])\n",
    "prediction = feature_model.predict(y)\n",
    "\n",
    "print(\"The following command belongs in cluster {}:\".format(prediction[0]))\n",
    "print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[Go to top](#top)\n",
    "\n",
    "### Prioritize  <a class=\"anchor\" id=\"prioritize\"></a>\n",
    "\n",
    "Now that we have a model with approximately 50 clusters, we can start prioritizing these clusters.  How should we prioritize?  The most obvious way would be by the number if bugs/features in each cluster, but there may be other factors like severity and the number of votes.  Let's take a look at the meta-data for a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example severity: S3\n",
      "Example voteCount: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Example severity: \" + features[0]['snapshot'][0]['severity'])\n",
    "print(\"Example voteCount: \" + str(features[0]['aggregatedData']['voteCount']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see properties like \"voteCount\" and \"severity\" that can be used to create some kind of index.  Another element we can use is age, but voteCount may be a function of age and time math is difficult, so let's try building an index using these two characteristics.\n",
    "\n",
    "First, the severity field appears to be a text field with categorical values.  We should find out what the range of those values are..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'S1', 'S2', 'S3', 'S4'}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set([x['snapshot'][-1]['severity'] for x in features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second, we should look at the range of voting values..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum voteCount value: 3793\n",
      "Minimum voteCount value: 0\n"
     ]
    }
   ],
   "source": [
    "print(\"Maximum voteCount value: \" + str(max([x['aggregatedData']['voteCount'] for x in features])))\n",
    "print(\"Minimum voteCount value: \" + str(min([x['aggregatedData']['voteCount'] for x in features])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, we need a way to convert a range between 0 and 3792 and a range between S1 and S4 to a \"score\" that can be used to produce an our own prioritization index.\n",
    "\n",
    "For the purpose of this exercise, let's assume the issue we are looking at has a severity of \"S2\" and a voteCount value of 1234."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "raw_severity_string = \"S2\"\n",
    "raw_vote_count = 1234"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to convert the severity scale from a 1(lowest) to 4(highest) to a numeric scale from 0-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_severity = int(raw_severity_string[-1])\n",
    "maximum_severity = 4\n",
    "minimum_severity = 1\n",
    "\n",
    "# Because we are working on a scale from 4-1 where 1 is the \"highest\" we need to invert the scale.\n",
    "adjusted_severity = maximum_severity + minimum_severity - raw_severity\n",
    "adjusted_severity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can plug the the voteCount and severity values into the following equation:\n",
    "\n",
    "(value - minimum) / (maximum - mininmum) * 100 = score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Severity score of 0.3333333333333333.\n",
      "Vote score of 0.0.\n"
     ]
    }
   ],
   "source": [
    "vote_max = 3783\n",
    "vote_min = 0\n",
    "my_vote_count = 0\n",
    "\n",
    "my_severity_score = (my_severity - minimum_severity) / (maximum_severity - minimum_severity)\n",
    "my_vote_score = (my_vote_count - vote_min) / (vote_max - vote_min)\n",
    "\n",
    "print(\"Severity score of {}.\".format(my_severity_score))\n",
    "print(\"Vote score of {}.\".format(my_vote_score))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpm",
   "language": "python",
   "name": "gpm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
